%! TEX root = **/000-main.tex
% vim: spell spelllang=en:

\section{Implementation}%
\label{sec:implementation}

The main result of this project is the program neuropy\cite{neuropy}.
The purpose of the program is to generate visualizations of the Hodgkin-Huxley model on neuron geometries utilizing extrapolation methods to minimize the impact of performance issues.
The code has two main steps; importing simulation data and generating the visualizations.
Since generating the simulation data on neuron geometries isn't the focus of the project, neuropy requires this information to be given in input files.
This can be done using Neuro-VISOR, but neuropy will take any pair of ``.csv'' and ``.swc'' files with corresponding indices.
It is designed to make comparison of different extrapolation methods and contexts easy and straightforward. 
All three of the linear function definitions explained in \cref{sec:extrapolation} are implemented, and new extrapolation methods can be added by simply adding a new python function.
For an example python function definition, see ``extrapolate'' within the code provided in the appendix.


\subsection{Importing Data}%
\label{sub:importing_data}
The simulation values are obtained from Neuro-VISOR\cite{neuroVISOR} and are stored in the ``.csv'' format where each row corresponds to one solve step.
The Neuron geometries are stored in the ``.swc'' file format\cite{swc} and were obtained from NeuroMorpho\cite{neuroMorpho}.
The code uses a package called ``Navis'' which handles neuron morphologies\cite{navis}.
It is important to note that the ``.swc'' file currently used in the code is actually obtained from Neuro-VISOR.
While Neuro-VISOR also obtains it's ``.swc'' files from NeuroMorpho but slightly alters them, changing the order of the node indexes\cite{neuropy}.
Neuro-VISOR doesn't provide these indices when exporting the simulation data so we are required to use it's neuron geometry files.

\subsection{Visualization}%
\label{sub:visualization}
There are multiple temporal parameters which should be properly disambiguated.
To begin, we assume that in the simulation data file each row is separated by a fixed time step.
This may be different than the time step of the solver, as is the case in Neuro-VISOR.
The most straight forward parameter in neuropy is the target fps, $f_t$, which is simply the frames per second (fps) of the output video.
This should be the first parameter chosen as the other's depend on it.
The next parameter is the rate of the simulation data, $f_d$ in fps.
Neuropy will read the next data point every $f_t / f_d$ frames.
$f_d$ should be a divisor of $f_t$ so none of the frames are hidden or removed.
Note that the playback speed will also be affected depending on $f_d / f_t$ and the size of the data's fixed time step.
Another parameter is the number of extrapolation points per $f_d$, denoted $f_e$ which determines how frequently the visualization is updated using the extrapolation values.
For example, if $f_t=60,f_d=1,f_e=10$ then every $6$ frames the visualization is updated.
Similar to $f_d$, $f_e$ should be a divisor of $f_t$ in order for the video to display every frame.
The last parameter is the number of data points skipped per read, $f_s$.
In other words, whenever a new data point is read it will skip the next $f_s - 1$ rows.
This is mainly used to easily create large differences between data points which can be useful for creating clear visualization examples.

\subsection{Results}%
\label{sub:results}
The main use of neuropy is to study how the extrapolation methods behave given various values for the parameters defined in \cref{sub:visualization}.
Neuropy includes $62$ examples with different parameter values and extrapolation methods.
However, due to the relationships between the parameters, the data, and the methods, it can be difficult to pick specific videos which demonstrate interesting differences.
To alleviate this we will list a few specific examples and what they demonstrate alongside the relevant file names in the neuropy repository\cite{neuropy}.
For all of the following examples, let $f_t=60$.
Also, if ``jump'' is $1$ the linear extrapolation with jumps is used, otherwise the linear extrapolation without jumps is used.
Constant extrapolation is created by making $f_e = 0$. 

\begin{table}[H]
    \begin{center}
    \caption{basicComparison\_center.mp4}%
    \label{tab:exampleOne}
    \begin{tabular}{c|c|c|c}
        \toprule
        $f_d$ & $f_s$ & $f_e$ & jump \\
        \midrule
        1 & 5 & 0 & 0 \\
        1 & 5 & 15 & 1 \\
        1 & 5 & 30 & 0 \\
        1 & 5 & 30 & 1 \\
        \bottomrule
    \end{tabular}
    \end{center}

\end{table}

The four configurations in \cref{tab:exampleOne} are chosen to show the difference between the three extrapolations defined in \cref{sub:optimal_methods} with different rates.
$f_s$ is set to $5$ to make the four options more easily distinguishable without getting rid of too many data points.
The first configuration is equivalent to constant extrapolation and is used as the baseline for the other three configurations.
The two configurations where $f_e = 30$ show the striking difference between the non-constant linear extrapolation methods, and the two configurations using linear extrapolation with jumps are only subtly different.

\begin{table}[H]
    \begin{center}
    \caption{jumpComparison\_full.mp4}%
    \label{tab:exampleTwo}
    \begin{tabular}{c|c|c|c}
        \toprule
        $f_d$ & $f_s$ & $f_e$ & jump \\
        \midrule
        15 & 1 & 0 & 0 \\
        1 & 15 & 0 & 0 \\
        1 & 15 & 30 & 0 \\
        1 & 15 & 30 & 1 \\
        \bottomrule
    \end{tabular}
    \end{center}

\end{table}

The four configurations in \cref{tab:exampleTwo} are more similar than in \cref{tab:exampleOne}.
They are specifically chosen to highlight how linear extrapolation without jumps behaves with extreme data.
All but the first configuration has $f_s=15$, which is very large but not so large that there is only one data point per oscillation.
The first two are baseline configurations without extrapolation, the first shows every datapoint and the second shows every fifteenth.
These are useful for visually judging the accuracy of the third and fourth configurations.
The corresponding video on neuropy also displays the L1 norm for the third and fourth configurations to get actual values for their accuracy.
Using both allows one to compare the accuracy with the more subjective ``feel'' of the extrapolation method.
The fourth configuration is the same as the third save the extrapolation method in order to show the difference between the methods.
The third configuration is impressively clear compared to the fourth configuration, however it is generally less accurate than the fourth configuration.


